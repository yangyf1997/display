---
title: "20191015学习报告"
date: 2019-10-14T16:09:50+08:00
---

#20191001-1015
## 机器学习算法-贝叶斯算法的学习
### 算法概述
* 贝叶斯公式  

```katex
\displaystyle
	P(A|B)=\frac{P(B|A)P(A)}{P(B)}
```  

* 贝叶斯公式所要解决的问题可以简单的理解为，用正向概率去求一个逆向概率。举个栗子，一个不透明的袋子里有m个白球和n个黑球，随便拿出来一个，白球的概率与黑球的概率很容易就能得到。但是若事先不知道白球与黑球的个数，只是通过观察拿出来的球的颜色，那么是否能对袋子里黑球与白球的个数作出一个推测。这个问题的解决在我们实际应用中是非常有必要性的。
* 在机器学习的视角下，我们把A理解成“类别标签”，把B理解成“具有某种特征”。这样贝叶斯公式可以理解为
```katex
\displaystyle
P("category"|"feature")=\frac{P("feature"|"category")P("category")}{P("feature")}
```  
***
### 贝叶斯应用-拼写纠错实例
#### 问题描述
* 假设用户输入了一个不在字典中的错误单词，我们需要为用户推荐一个正确单词以推测其真正想输入的单词
* P（猜测用户想输入的单词是什么|实际输入的单词）  

#### 解决过程
* 用户实际输入的单词记为D（D代表Data，即观测数据）
* 猜测：
	* 猜测1：P(h1|D)
	* 猜测2：P(h2|D)
	* 猜测3：P(h3|D)
	* ...
* 统一为：P(h|D)
```katex
\displaystyle
	P(h|D)=\frac{P(D|h)P(h)}{P(D)}
```
* 对于给定观测数据，一个猜测是好是坏，就是P(h|D)的大小。对于不同的具体猜测h1h2h3...，P(D)都是一样的，所以在比较时我们可以忽略这个常数。所以猜测的好坏就取决于P(h)"这个猜测本身独立的可能性大小"和P(D|h)"这个猜测生成我们猜测到的数据的可能性大小"
* P(h|D)正比于P(D|h)和P(h)  

```katex
\displaystyle
	P(h|D)\approx{P(D|h)P(h)}
```
* 其中P(h)是特定的先验概率，比如此处假定h这个猜测的单词在单词库所有单词中占的比重，即词频
* P(D|h)是指通过某些衡量指标来体现这个猜测生成我们猜测到的数据的可能性大小，比如此处可以采用，键盘上字母之间的距离来体现其可能性的大小，距离越近可能性越大，反之则越小
* 那么现在P(h|D)就可以很好地进行比较了，值越大说明猜测越准确，就越往前推荐，这样就完成拼写纠错的目的

